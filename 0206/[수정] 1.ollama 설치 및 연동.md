## 0. 올라마 설치 
공식 홈페이지
https://brunch.co.kr//@b2439ea8fc654b8/66
<img width="940" height="193" alt="image" src="https://github.com/user-attachments/assets/38eca3b9-b72f-4754-bfd9-9bab38619882" />


- cmd+R로 프롬프트를 열어 ollama --version 명령어를 통해 설치 여부를 확인.
<img width="703" height="141" alt="image" src="https://github.com/user-attachments/assets/81a7871e-f753-4d84-97ac-6ec3ac6937a6" />

--------------------------------------------

## 1. 모델 다운로드
- 많은 후보군 중에서 노트북 사양에 적합하고 성능이 좋은 gemma3:4b를 선택하여 다운로드
- 40억(4B) 파라미터로 모델의 지능/표현력 크기를 의미한다. 
- 올라마 프롬프트에서 해도 설치가 되었지만, 앞선 프롬프트 창에서 다운받는 것을 추천.
<img width="940" height="72" alt="image" src="https://github.com/user-attachments/assets/c50aad67-dfb8-43ce-ad22-4813ff53a94b" />

- ollama list 명령어를 통해 모델 다운로드 여부를 확인
- 모델의 NAME 영역이 앞으로 모델 run할 때도 사용된다.
<img width="940" height="113" alt="image" src="https://github.com/user-attachments/assets/98e10de7-2781-465b-b6c7-1c8d03029238" />

-------------------------------------------
## 2. API 연결
- ollama run gemma3:4b로 모델을 실행하여 로컬 서버를 활성화
<img width="805" height="86" alt="image" src="https://github.com/user-attachments/assets/13706e52-73d0-4c3d-aabf-b94fbd9df11e" />

- Ollama는 기본적으로 11434 포트를 사용합니다. 브라우저에서 연결을 확인한다.
<img width="673" height="228" alt="image" src="https://github.com/user-attachments/assets/200a8a4a-c0ce-4f66-a50d-076247b3f8f0" />

- 네트워크 연결이 끊겨도 postman으로 답을 받을 수 있는지 확인한다.
- 아래와 같이 JSON 응답이 정상적으로 출력되면 API 연결이 완료된 것입니다.
<img width="738" height="429" alt="image" src="https://github.com/user-attachments/assets/6ac01a42-3564-409f-abf3-5ac32dce7d24" />


-----------------------------------------
## 3. 앞으로의 방향성
- 다운로드된 gemma랑 큐원의 성능을 비교해본다.
- 로컬 파인튜닝을 하여 챗봇을 도입한 영어 회화 서비스를 구현해본다.
